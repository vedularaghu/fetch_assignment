{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: tensor([[-0.1373, -0.1593,  0.0821,  ..., -0.0644, -0.0986, -0.0170],\n",
      "        [ 0.0211, -0.0120, -0.1718,  ..., -0.1398, -0.1174,  0.2579]])\n",
      "Sentence Embeddings Shape: torch.Size([2, 768])\n",
      "Sentence: 'Hello World!' | Embedding (first 5 values): tensor([-0.1373, -0.1593,  0.0821, -0.3459, -0.2501])\n",
      "Sentence: 'I'm AGI! How can I help you?' | Embedding (first 5 values): tensor([ 0.0211, -0.0120, -0.1718, -0.3703,  0.1255])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        # Load pre-trained BERT model and tokenizer\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get token embeddings from BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Mean pooling: compute the mean across the sequence length dimension\n",
    "        # Use attention_mask to exclude padding tokens from the mean\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)  # Avoid division by zero\n",
    "        sentence_embeddings = sum_embeddings / sum_mask  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        return sentence_embeddings\n",
    "\n",
    "    def encode(self, sentences):\n",
    "        # Tokenize input sentences\n",
    "        encoding = self.tokenizer(sentences, padding=True, truncation=True, \n",
    "                                 max_length=128, return_tensors='pt')\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        # Forward pass to get embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.forward(input_ids, attention_mask)\n",
    "        return embeddings\n",
    "\n",
    "# Test the implementation\n",
    "model = SentenceTransformer()\n",
    "sample_sentences = [\n",
    "    \"Hello World!\",\n",
    "    \"I'm AGI! How can I help you?\"\n",
    "]\n",
    "embeddings = model.encode(sample_sentences)\n",
    "print(\"Embeddings:\", embeddings)\n",
    "print(\"Sentence Embeddings Shape:\", embeddings.shape)  # Expected: (2, 768)\n",
    "for sentence, emb in zip(sample_sentences, embeddings):\n",
    "    print(f\"Sentence: '{sentence}' | Embedding (first 5 values): {emb[:5]}\")\n",
    "\n",
    "# Explanation:\n",
    "\n",
    "# Transformer Backbone: I chose BERT (bert-base-uncased) because it’s a widely-used, pre-trained transformer model \n",
    "# that captures rich contextual information, making it suitable as a foundation for sentence embeddings. \n",
    "# Using a pre-trained model saves time compared to training a transformer from scratch and leverages BERT’s general language understanding.\n",
    "\n",
    "#Pooling Strategy: To convert token-level embeddings from BERT into a single sentence embedding, \n",
    "# I used mean pooling over the token embeddings which aggregates information from all tokens, \n",
    "# weighted by the attention mask to ignore padding, and is a common choice in sentence transformers\n",
    "# (e.g., Sentence-BERT) because it often outperforms using the [CLS] token alone for sentence-level tasks.\n",
    "\n",
    "# I didn’t add a projection layer after pooling to keep the architecture simple and preserve the \n",
    "# 768-dimensional embeddings from BERT, which are already rich and usable for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Logits Shape: torch.Size([2, 2])\n",
      "NER Logits Shape: torch.Size([2, 10, 5])\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_sentence_classes=2, num_ner_classes=5, model_name='bert-base-uncased'):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        # Shared BERT backbone\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size  # 768 for bert-base\n",
    "        \n",
    "        # Sentence classification head\n",
    "        self.sentence_classifier = nn.Linear(hidden_size, num_sentence_classes)\n",
    "        \n",
    "        # NER classification head\n",
    "        self.ner_classifier = nn.Linear(hidden_size, num_ner_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get outputs from BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Sentence classification: mean pooling\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "        sentence_embeddings = sum_embeddings / sum_mask\n",
    "        sentence_logits = self.sentence_classifier(sentence_embeddings)  # Shape: (batch_size, num_sentence_classes)\n",
    "        \n",
    "        # NER classification: per-token predictions\n",
    "        ner_logits = self.ner_classifier(last_hidden_state)  # Shape: (batch_size, seq_len, num_ner_classes)\n",
    "        \n",
    "        return sentence_logits, ner_logits\n",
    "\n",
    "# Test the model\n",
    "model = MultiTaskModel(num_sentence_classes=2, num_ner_classes=5)\n",
    "sample_sentences = [\"Let's get this party started!\", \"Humans will always win against AGI\"]\n",
    "encoding = model.tokenizer(sample_sentences, padding=True, truncation=True, \n",
    "                          max_length=128, return_tensors='pt')\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "sentence_logits, ner_logits = model(input_ids, attention_mask)\n",
    "print(\"Sentence Logits Shape:\", sentence_logits.shape)\n",
    "print(\"NER Logits Shape:\", ner_logits.shape) \n",
    "\n",
    "# Shared Backbone: The BERT model remains shared to learn representations beneficial for both tasks\n",
    "# Sentence Classification Head: Takes the pooled sentence embedding and outputs logits for 2 classes (e.g. +/-). I assumed a binary classification task for simplicity.\n",
    "# NER Head: Operates on the full last_hidden_state to produce per-token logits for 5 NER classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = MultiTaskModel(num_sentence_classes=2, num_ner_classes=5)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Hypothetical training loop\n",
    "def train_epoch(model, dataloader):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids']  # Shape: (batch_size, seq_len)\n",
    "        attention_mask = batch['attention_mask']  # Shape: (batch_size, seq_len)\n",
    "        sentence_labels = batch['sentence_labels']  # Shape: (batch_size)\n",
    "        ner_labels = batch['ner_labels']  # Shape: (batch_size, seq_len), -100 for padding\n",
    "        \n",
    "        # Forward pass\n",
    "        sentence_logits, ner_logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Compute losses\n",
    "        sentence_loss = F.cross_entropy(sentence_logits, sentence_labels)\n",
    "        ner_loss = F.cross_entropy(ner_logits.view(-1, num_ner_classes), \n",
    "                                  ner_labels.view(-1), ignore_index=-100)\n",
    "        total_loss = sentence_loss + ner_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute metrics\n",
    "        sentence_preds = torch.argmax(sentence_logits, dim=1)\n",
    "        sentence_acc = (sentence_preds == sentence_labels).float().mean()\n",
    "        ner_preds = torch.argmax(ner_logits, dim=-1)\n",
    "        ner_mask = (ner_labels != -100)\n",
    "        ner_acc = (ner_preds[ner_mask] == ner_labels[ner_mask]).float().mean()\n",
    "        \n",
    "        print(f\"Sentence Loss: {sentence_loss.item():.4f}, NER Loss: {ner_loss.item():.4f}, \"\n",
    "              f\"Sentence Acc: {sentence_acc.item():.4f}, NER Acc: {ner_acc.item():.4f}\")\n",
    "\n",
    "# Note: In practice, run `for epoch in range(num_epochs): train_epoch(model, dataloader)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
